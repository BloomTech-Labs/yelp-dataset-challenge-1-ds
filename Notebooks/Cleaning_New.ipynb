{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Up: \n",
    "Before Running the cell below, you must ensure that these have been run in Terminal **IN ORDER** : \n",
    "- conda update -n base -c defaults conda \n",
    "\n",
    "    - cd SageMaker\n",
    "    \n",
    "      - cd yelp-dataset-challenge-1-ds\n",
    "      \n",
    "         - conda env create -f environment.yml\n",
    "          \n",
    "            - source activate ydc1 \n",
    "                \n",
    "                - pip install python-decouple\n",
    "                  \n",
    "                  - pip install pprintpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['API/',\n",
       " 'API/api.py',\n",
       " 'API/api_exploration.ipynb',\n",
       " 'Environments/',\n",
       " 'Environments/environment.yml',\n",
       " 'Flask_App/',\n",
       " 'Flask_App/Pipfile',\n",
       " 'Flask_App/__init__.py',\n",
       " 'Flask_App/app.py',\n",
       " 'Flask_App/models.py',\n",
       " 'Flask_App/yelp.py',\n",
       " 'Model/',\n",
       " 'Model/vect_1.sav',\n",
       " 'datasets/',\n",
       " 'datasets/df.csv',\n",
       " 'datasets/dtm.csv',\n",
       " 'datasets/dtm_final.csv',\n",
       " 'notebooks/',\n",
       " 'notebooks/data_cleanup.ipynb',\n",
       " 'notebooks/official_NB.ipynb',\n",
       " 'notebooks/vectorization_exploration.ipynb',\n",
       " 'notebooks/yelp_data_initial_exploration.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import s3\n",
    "from pprintpp import pprint as pp\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "\n",
    "# Load in Bucket\n",
    "bucket = s3.Bucket('yelpchallenge1')\n",
    "\n",
    "# Look inside Bucket \n",
    "bucket.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    ### ***** DO NOT RUN. ******* #### \n",
    "                  ### ***** ALREADY INSTALLED. ****** ###\n",
    "\n",
    "    # Installs the File 'Locally' on SageMaker Instance / Only have to run these once: \n",
    "\n",
    "bucket.get('datasets/df.csv', 'df.csv')\n",
    "\n",
    "    # Installing .json Files 'Locally'\n",
    "    \n",
    "bucket.get('datasets/user.json', 'user.json')\n",
    "bucket.get('datasets/review.json', 'review.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data: Complete as of ***8:14 PM : 12/19/2019***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning df.csv & saving Cleaned df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    ### ***** DO NOT RUN. ******* #### \n",
    "                  ### ***** ALREADY COMPLETE. ****** ###\n",
    "# Further Cleaning of df.csv: \n",
    "# Import \n",
    "df = pd.read_csv('df.csv')\n",
    "# Dropping Columns:\n",
    "#df = df.drop(columns=['Unnamed: 0', 'stars'])\n",
    "\n",
    "# Dropping all Missing / Na Values from Entire Dataframe:\n",
    "df = df.dropna()\n",
    "\n",
    "    # Saving Cleaned df.csv \n",
    "df.to_csv(index=True)\n",
    "df.to_csv(r'df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting user_json & to Pandas DataFrame / Saving as user.csv & review.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                              # ******* DO NOT RUN! ******* # \n",
    "                            # ***** ALREADY COMPLETE. ****** # \n",
    " # import user.json\n",
    "with open('user.json') as f:\n",
    "    user = json.loads(\"[\" + \n",
    "                      f.read().replace(\"}\\n{\", \"},\\n{\") + \n",
    "                      \"]\")\n",
    "    \n",
    "    # convert user.json files to pandas DataFrame 'user_df'\n",
    "user_df = pd.DataFrame(user)\n",
    "\n",
    "    # Saving user_df as csv file. \n",
    "user_df.to_csv(index=True)\n",
    "user_df.to_csv(r'user.csv')\n",
    "\n",
    "# Import review.json \n",
    "with open('review.json') as f:\n",
    "    review = json.loads(\"[\" + \n",
    "                        f.read().replace(\"}\\n{\", \"},\\n{\") + \n",
    "                        \"]\")\n",
    "    \n",
    "    # convert review.json files to pandas DataFrame 'review_df'\n",
    "review_df = pd.DataFrame(review)\n",
    "\n",
    "    # Saving user_df as csv file. \n",
    "review_df.to_csv(index=True)\n",
    "review_df.to_csv(r'review.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Merging: Complete as of ***8:14 PM : 12/19/2019***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # ***** New DTM DF HAS BEEN CREATED. DO NOT RUN THIS CELL **** #\n",
    "# Read-in dtm.csv (Original)\n",
    "dtm = pd.read_csv('dtm.csv')\n",
    "\n",
    "    # Taking Stars Column\n",
    "stars = df['stars']\n",
    "\n",
    "    # Adding stars column to dtm\n",
    "dtm['stars']=df['stars']\n",
    "\n",
    "# Shifting 'Stars' Column to front of Df,\n",
    "cols = list(dtm.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "dtm = dtm[cols]\n",
    "\n",
    "    # Dropping \"-PRON-\", 'year -PRON-', and ' ' Columns\n",
    "dtm = dtm.drop(columns=[' ', '  -PRON-', 'year -PRON-'])#Cut 135,000 Rows of df['stars'] Column to fix Memory Error. \n",
    "    # Label as \"stars\"\n",
    "stars = df.stars[0:135000]\n",
    "stars.shape\n",
    "    # Adding stars to dtm2\n",
    "dtm2['stars']=df['stars'][0:135000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "             # ***** New DTM2 DF HAS BEEN CREATED. DO NOT RUN THIS CELL **** #\n",
    "    \n",
    "    # Read-in dtm2.csv(Old)\n",
    "dtm2 = pd.read_csv('dtm2.csv')\n",
    "\n",
    "    # Taking Stars Column\n",
    "stars = df['stars']\n",
    "\n",
    "    # Adding stars column to dtm\n",
    "dtm2['stars']=df['stars']\n",
    "\n",
    "    # Shifting 'Stars' Column to front of Df,\n",
    "cols = list(dtm2.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "dtm2 = dtm2[cols]\n",
    "\n",
    "dtm2 = dtm2.drop(columns=['stars'])\n",
    "\n",
    "    # Dropping columns: \n",
    "dtm2 = dtm2.drop(columns=[' ' , '  '])\n",
    "dtm2 = dtm2.drop(columns=['  -PRON-','  i',  '  the',  '  this', '$', \"'s\"])\n",
    "    # Saving dtm2.csv \n",
    "dtm2.to_csv(index=True)\n",
    "dtm2.to_csv(r'dtm2.csv')\n",
    "\n",
    "    # Cut 135,000 Rows of df['stars'] Column to fix Memory Error. \n",
    "        # Label as \"stars\"     \n",
    "stars = df.stars[0:135000]\n",
    "stars.shape\n",
    "\n",
    "        # Adding stars to dtm2\n",
    "dtm2['stars']=df['stars'][0:135000]\n",
    "\n",
    "    # Saving Final df as 'dtm_final'\n",
    "dtm_final = dtm2\n",
    "\n",
    "    # Saving dtm_final.csv \n",
    "dtm_final.to_csv(index=True)\n",
    "dtm_final.to_csv(r'dtm_final.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in dtm_final.csv (FINAL)\n",
    "#dtm_final = pd.read_csv('dtm_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean / Analyze user.csv: Complete as of ***10:14 PM 12/19/2019***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "    # Read-in user.csv\n",
    "user = pd.read_csv('user.csv')\n",
    "    # Read-in review.csv\n",
    "review = pd.read_csv('review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Read-in of df_user\n",
    "# Checking Null Values and Shape\n",
    "pp(user.isna().sum())\n",
    "pp(user.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three Problems: \n",
    "**Problem 1:**\n",
    "user['Unamed: 0'] should not exist. \n",
    "\n",
    "**Problem 2:**\n",
    "user['elite'] has 1,565,761 Missing Values. \n",
    "\n",
    "**Problem 3:** \n",
    "user['name'] has 3 Missing Values. \n",
    "\n",
    "**Solution?:**\n",
    "Drop user['Unamed: 0' , 'elite']  Columns.\n",
    "\n",
    "Drop Missing Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "user = user.drop(columns=['Unnamed: 0', 'elite' ])\n",
    "user = user.dropna()\n",
    "# Save Cleaned user_df.csv \n",
    "user.to_csv(index=True)\n",
    "user.to_csv(r'user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unused columns from user_df\n",
    "user = user.drop(columns=['average_stars', 'compliment_cool', 'compliment_cute',\n",
    "       'compliment_funny', 'compliment_hot', 'compliment_list',\n",
    "       'compliment_more', 'compliment_note', 'compliment_photos',\n",
    "       'compliment_plain', 'compliment_profile', 'compliment_writer', 'cool', 'friends', 'funny', 'useful'])\n",
    "\n",
    "# Save Cleaned user_df.csv \n",
    "user.to_csv(index=True)\n",
    "user.to_csv(r'user.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean / Analyze review.csv: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Read-in of review.csv\n",
    "# Checking Null Values and Shape\n",
    "pp(review.isna().sum())\n",
    "pp(review.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minor Problem(s) with a Simple Solution**\n",
    "\n",
    "**Problems?:** \n",
    "review['date', 'funny', 'review_id', 'stars', 'text', 'useful'] Columns have NaN's. \n",
    "\n",
    "review['Unnamed: 0'] Not Supposed to be there.\n",
    "\n",
    "**Solution?:**\n",
    "Drop Missing Values from review DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: \n",
    "review = review.dropna()\n",
    "review = review.drop(columns=['Unnamed: 0', 'stars', 'business_id'])\n",
    "# Save Cleaned review_df.csv \n",
    "review.to_csv(index=True)\n",
    "review.to_csv(r'review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = review.drop(columns=['text'])\n",
    "review.to_csv(index=True)\n",
    "review.to_csv(r'review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding df['text', 'tokens'] to review.csv\n",
    "review['text'] = df['text']\n",
    "review['tokens'] = df['tokens']\n",
    "#review = review.drop(columns=['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining review.csv, & df.csv \n",
    "\n",
    "**Description**: \n",
    "\n",
    "Combining based on their *Unique Account ID's.*\n",
    "The end Product will be One DataFrame Consisting of Each Account:\n",
    "- **Name**, \n",
    "- **User_ID**,\n",
    "- **Review_ID**,\n",
    "- **Text**,\n",
    "- **That Users respective review(s)**,\n",
    "- **Interactions that Review (i.e: Cool, Funny, Useful)**  \n",
    "\n",
    "The goal of the model is to have the ability to type in the Review you are wanting to post on Yelp, and give the User the ability to Predict What type of Interaction they would potentially receive and Total Number of each interaction. The model Accuracy will be Displayed beside the Prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing Layout of Columns \n",
    "final_df = review[['user_id', 'date', 'review_id', 'useful', 'funny', 'cool', 'text']]\n",
    "\n",
    "#Saving Final_df\n",
    "final_df.to_csv(index=True)\n",
    "final_df.to_csv(r'final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Null Values and Shape\n",
    "pp(final_df.isna().sum())\n",
    "pp(final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Null Values from [text] column\n",
    "final = final_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Null Values and Shape\n",
    "pp(final.isna().sum())\n",
    "pp(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Final\n",
    "final.to_csv(index=True)\n",
    "final.to_csv(r'final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['cool'] = final.cool.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Final\n",
    "final.to_csv(index=True)\n",
    "final.to_csv(r'final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.read_csv('final.csv') \n",
    "final = final.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'date', 'review_id', 'useful', 'funny', 'cool', 'text',\n",
       "       'tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Final\n",
    "final.to_csv(index=True)\n",
    "final.to_csv(r'final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Visulizations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Code for hiding seaborn warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sns.distplot(final['useful']).set_title('Useful Interaction Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_ydc1",
   "language": "python",
   "name": "conda_ydc1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
