{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Up: \n",
    "Before Running the cell below, you must ensure that these have been run in Terminal **IN ORDER** : \n",
    "- conda update -n base -c defaults conda \n",
    "\n",
    "    - cd SageMaker\n",
    "    \n",
    "      - cd yelp-dataset-challenge-1-ds\n",
    "      \n",
    "         - conda env create -f environment.yml\n",
    "          \n",
    "            - source activate ydc1 \n",
    "                \n",
    "                - pip install python-decouple\n",
    "                  \n",
    "                  - pip install pprintpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['API/',\n",
       " 'API/api.py',\n",
       " 'API/api_exploration.ipynb',\n",
       " 'Environments/',\n",
       " 'Environments/environment.yml',\n",
       " 'Flask_App/',\n",
       " 'Flask_App/Pipfile',\n",
       " 'Flask_App/__init__.py',\n",
       " 'Flask_App/app.py',\n",
       " 'Flask_App/models.py',\n",
       " 'Flask_App/yelp.py',\n",
       " 'Model/',\n",
       " 'Model/vect_1.sav',\n",
       " 'datasets/',\n",
       " 'datasets/df.csv',\n",
       " 'datasets/dtm.csv',\n",
       " 'datasets/dtm_final.csv',\n",
       " 'datasets/review.json',\n",
       " 'datasets/user.json',\n",
       " 'notebooks/',\n",
       " 'notebooks/data_cleanup.ipynb',\n",
       " 'notebooks/official_NB.ipynb',\n",
       " 'notebooks/vectorization_exploration.ipynb',\n",
       " 'notebooks/yelp_data_initial_exploration.ipynb']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import s3\n",
    "from pprintpp import pprint as pp\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "\n",
    "# Load in Bucket\n",
    "bucket = s3.Bucket('yelpchallenge1')\n",
    "\n",
    "# Look inside Bucket \n",
    "bucket.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    ### ***** DO NOT RUN. ******* #### \n",
    "                  ### ***** ALREADY INSTALLED. ****** ###\n",
    "\n",
    "    # Installs the File 'Locally' on SageMaker Instance / Only have to run these once: \n",
    "\n",
    "#bucket.get('datasets/df.csv', 'df.csv')\n",
    "\n",
    "    # Installing .json Files 'Locally'\n",
    "    \n",
    "#bucket.get('datasets/user.json', 'user.json')\n",
    "#bucket.get('datasets/review.json', 'review.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data: Complete as of ***8:14 PM : 12/19/2019***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning df.csv & saving Cleaned df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    ### ***** DO NOT RUN. ******* #### \n",
    "                  ### ***** ALREADY COMPLETE. ****** ###\n",
    "# Further Cleaning of df.csv: \n",
    "\n",
    "# Dropping Column:\n",
    "#df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Dropping all Missing / Na Values from Entire Dataframe:\n",
    "#df = df.dropna()\n",
    "\n",
    "    # Saving Cleaned df.csv \n",
    "#df.to_csv(index=False)\n",
    "#df.to_csv(r'df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting user_json & to Pandas DataFrame / Saving as user.csv & review.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "                              # ******* DO NOT RUN! ******* # \n",
    "                            # ***** ALREADY COMPLETE. ****** # \n",
    " # import user.json\n",
    "#with open('user.json') as f:\n",
    "    #user = json.loads(\"[\" + \n",
    "                      #f.read().replace(\"}\\n{\", \"},\\n{\") + \n",
    "                      #\"]\")\n",
    "    \n",
    "    # convert user.json files to pandas DataFrame 'user_df'\n",
    "#user_df = pd.DataFrame(user)\n",
    "\n",
    "    # Saving user_df as csv file. \n",
    "#user_df.to_csv(index=False)\n",
    "#user_df.to_csv(r'user.csv')\n",
    "\n",
    "# Import review.json \n",
    "#with open('review.json') as f:\n",
    "    #review = json.loads(\"[\" + \n",
    "                       # f.read().replace(\"}\\n{\", \"},\\n{\") + \n",
    "                        #\"]\")\n",
    "    \n",
    "    # convert review.json files to pandas DataFrame 'review_df'\n",
    "#review_df = pd.DataFrame(review)\n",
    "\n",
    "    # Saving user_df as csv file. \n",
    "#review_df.to_csv(index=False)\n",
    "#review_df.to_csv(r'review.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Merging: Complete as of ***8:14 PM : 12/19/2019***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # ***** New DTM DF HAS BEEN CREATED. DO NOT RUN THIS CELL **** #\n",
    "# Read-in dtm.csv (Original)\n",
    "#dtm = pd.read_csv('dtm.csv')\n",
    "\n",
    "    # Taking Stars Column\n",
    "#stars = df['stars']\n",
    "\n",
    "    # Adding stars column to dtm\n",
    "#dtm['stars']=df['stars']\n",
    "\n",
    "# Shifting 'Stars' Column to front of Df,\n",
    "#cols = list(dtm.columns)\n",
    "#cols = [cols[-1]] + cols[:-1]\n",
    "#dtm = dtm[cols]\n",
    "\n",
    "    # Dropping \"-PRON-\", 'year -PRON-', and ' ' Columns\n",
    "#dtm = dtm.drop(columns=[' ', '  -PRON-', 'year -PRON-'])#Cut 135,000 Rows of df['stars'] Column to fix Memory Error. \n",
    "    # Label as \"stars\"\n",
    "#stars = df.stars[0:135000]\n",
    "#stars.shape\n",
    "    # Adding stars to dtm2\n",
    "#dtm2['stars']=df['stars'][0:135000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "             # ***** New DTM2 DF HAS BEEN CREATED. DO NOT RUN THIS CELL **** #\n",
    "    \n",
    "    # Read-in dtm2.csv(Old)\n",
    "#dtm2 = pd.read_csv('dtm2.csv')\n",
    "\n",
    "    # Taking Stars Column\n",
    "#stars = df['stars']\n",
    "\n",
    "    # Adding stars column to dtm\n",
    "#dtm2['stars']=df['stars']\n",
    "\n",
    "    # Shifting 'Stars' Column to front of Df,\n",
    "#cols = list(dtm2.columns)\n",
    "#cols = [cols[-1]] + cols[:-1]\n",
    "#dtm2 = dtm2[cols]\n",
    "\n",
    "#dtm2 = dtm2.drop(columns=['stars'])\n",
    "\n",
    "    # Dropping columns: \n",
    "#dtm2 = dtm2.drop(columns=[' ' , '  '])\n",
    "#dtm2 = dtm2.drop(columns=['  -PRON-','  i',  '  the',  '  this', '$', \"'s\"])\n",
    "    # Saving dtm2.csv \n",
    "#dtm2.to_csv(index=False)\n",
    "#dtm2.to_csv(r'dtm2.csv')\n",
    "\n",
    "    # Cut 135,000 Rows of df['stars'] Column to fix Memory Error. \n",
    "        # Label as \"stars\"     \n",
    "#stars = df.stars[0:135000]\n",
    "#stars.shape\n",
    "\n",
    "        # Adding stars to dtm2\n",
    "#dtm2['stars']=df['stars'][0:135000]\n",
    "\n",
    "    # Saving Final df as 'dtm_final'\n",
    "#dtm_final = dtm2\n",
    "\n",
    "    # Saving dtm_final.csv \n",
    "#dtm_final.to_csv(index=False)\n",
    "#dtm_final.to_csv(r'dtm_final.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in dtm_final.csv (FINAL)\n",
    "#dtm_final = pd.read_csv('dtm_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean / Analyze user.csv & review.csv: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "    # Read-in user.csv\n",
    "user = pd.read_csv('user.csv')\n",
    "    # Read-in review.csv\n",
    "review = pd.read_csv('review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                  0\n",
      "average_stars               0\n",
      "compliment_cool             0\n",
      "compliment_cute             0\n",
      "compliment_funny            0\n",
      "compliment_hot              0\n",
      "compliment_list             0\n",
      "compliment_more             0\n",
      "compliment_note             0\n",
      "compliment_photos           0\n",
      "compliment_plain            0\n",
      "compliment_profile          0\n",
      "compliment_writer           0\n",
      "cool                        0\n",
      "elite                 1565761\n",
      "fans                        0\n",
      "friends                     0\n",
      "funny                       0\n",
      "name                        3\n",
      "review_count                0\n",
      "useful                      0\n",
      "user_id                     0\n",
      "yelping_since               0\n",
      "dtype: int64\n",
      "(1637138, 23)\n"
     ]
    }
   ],
   "source": [
    "# Check Read-in of df_user\n",
    "# Checking Null Values and Shape\n",
    "pp(user.isna().sum())\n",
    "pp(user.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three Problems: \n",
    "**Problem 1:**\n",
    "user['Unamed: 0'] should not exist. \n",
    "\n",
    "**Problem 2:**\n",
    "user['elite'] has 1,565,761 Missing Values. \n",
    "\n",
    "**Problem 3:** \n",
    "user['name'] has 3 Missing Values. \n",
    "\n",
    "**Solution?:**\n",
    "Drop user['Unamed: 0' , 'elite' , 'name']  Columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "user_df = user.drop(columns=['Unnamed: 0', 'elite' , 'name'])\n",
    "\n",
    "# Save Cleaned user_df.csv \n",
    "user_df.to_csv(index=False)\n",
    "user_df.to_csv(r'user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_ydc1",
   "language": "python",
   "name": "conda_ydc1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
